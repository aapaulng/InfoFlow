\documentclass[12pt,a4paper]{article}
\title{InfoFlow Maths}
\author{Felix Fung}
\date{\today}

\usepackage{amsmath}
\usepackage{amssymb}

\begin{document}

\maketitle

\begin{abstract}
This document contains the relevant discrete maths and algorithmic considerations involved in the InfoMap and its parallelized algorithm, InfoFlow. We first provide a very brief overview of the InfoMap algorithm and relevant maths, then we derive discrete maths for the InfoMap algorithm specifically useful in a Spark implementation. We then expand the maths for a greedy algorithm that allows for improved performance and discuss the algorithm, which we call InfoFlow.
\end{abstract}

\tableofcontents

\section{Fundamentals}

These are the fundamental maths found in the original paper.

\subsection{Nodes}

Each node is indexed, with the index denoted by greek alphabets \(\alpha,\beta\) or \(\gamma\). Each node \(\alpha\) is associated with an ergodic frequency \(p_\alpha\).

In between nodes there may be a directed edge \(\omega_{\alpha\beta}\) from node \(\alpha\) to node \(\beta\). The directed edge weights are normalized with respect to the outgoing node, so that
\[
    \sum_\alpha \omega_{\alpha\beta} = 1
\]

The nodes are unchanged for all partitioning schemes.

\subsection{Modules}

The nodes are partitioned into modules. Each module is indexed with Latin alphabets \(i\), \(j\), or \(k\).

Each module has ergodic frequency:
\begin{equation}
    p_i = \sum_{\alpha\in i}p_\alpha
\end{equation}
and probability of exiting the module is:
\begin{equation}
    q_i = \tau\frac{n-n_i}{n-1}p_i +(1-\tau)\sum_{\alpha\in i}\sum_{\beta\notin i}p_\alpha\omega_{\alpha\beta}
\label{eq:q}
\end{equation}
with these, we try to minimize the code length:
\begin{equation}
    L =
    \mathrm{plogp}\left( \sum_iq_i \right)
    -2\sum_i\mathrm{plogp}\left(q_i\right)
    -\sum_\alpha \mathrm{plogp}(p_\alpha)
    +\sum_i\mathrm{plogp}\left( p_i+q_i \right)
\label{eq:L}
\end{equation}
where
\begin{equation}
    \mathrm{plogp}(x) = x \mathrm{log}_2 x
\end{equation}

\section{Simplifying calculations}

We develop maths to reduce the computational complexity for merging calculations. Specifically, we find recursive relations, so that when we merge modules \(j\) and \(k\) into \(i\), we calculate the properties of \(i\) from those of \(j\) and \(k\).

\subsection{Calculating merging quantities}

We can rewrite Eq.~(\ref{eq:q}) as
\begin{equation}
    q_i = \tau\frac{n-n_i}{n-1}p_i +(1-\tau)w_i
\label{eq:q-simplified}
\end{equation}
with
\begin{equation}
    w_i = \sum_{\alpha\in i}\sum_{\beta\notin i}p_\alpha\omega_{\alpha\beta}
\end{equation}
being the exit probability without teleportation.

We can define a similar quantity, the transition probability without teleportation from module \(j\) to module \(k\):
\begin{equation}
    w_{jk} = \sum_{\alpha\in j}\sum_{\beta\in k}p_\alpha\omega_{\alpha\beta}
\end{equation}

Now, if we merge modules \(j\) and \(k\) into into a new module with index \(i\), the exit probability would be follow Eq.~(\ref{eq:q-simplified}) with
\begin{align}
    n_i &= n_j +n_k\\
    p_i &= p_j +p_k
\end{align}
and the exit probability without teleportation can be calculated via:
\begin{align}
    w_i &= \sum_{\alpha\in i} \sum_{\beta\notin i} p_\alpha\omega_{\alpha\beta}\\
        &= \sum_{\substack{~~~\alpha\in j\\\mathrm{or}~\alpha\in k}} ~~ \sum_{\substack{~~~~\beta\notin j\\\mathrm{and}\beta\notin k}} p_\alpha\omega_{\alpha\beta}\\
        &= \sum_{\alpha\in j} ~~ \sum_{\substack{~~~~\beta\notin j\\\mathrm{and}\beta\notin k}} p_\alpha\omega_{\alpha\beta}
          +\sum_{\alpha\in k} ~~ \sum_{\substack{~~~~\beta\notin j\\\mathrm{and}\beta\notin k}} p_\alpha\omega_{\alpha\beta}
\end{align}
since we are looking at the exit probability of a module, there are no self connections within modules, so that the specification of \(p_\alpha w_{\alpha\beta}\) given \(\alpha\in i\), \(\beta\notin i\) is redundant. Then we have
\begin{equation}
    w_i = \sum_{\alpha\in j} \sum_{\beta\notin k} p_\alpha\omega_{\alpha\beta}
          +\sum_{\alpha\in k} \sum_{\beta\notin j} p_\alpha\omega_{\alpha\beta}
\end{equation}
which conforms with intuition, that the exit probability without teleportation of the new module is equal to the exit probability of all nodes without counting for the connections from \(j\) to \(k\), or from \(k\) to \(j\).

We can further simplify the maths by expanding the non-inclusive set specification:
\begin{equation}
    w_i = \sum_{\alpha\in j} \left[ \sum_\beta p_\alpha\omega_{\alpha\beta} -\sum_{\beta\in k} p_\alpha\omega_{\alpha\beta} \right]
          +\sum_{\alpha\in k} \left[ \sum_\beta p_\alpha\omega_{\alpha\beta} -\sum_{\beta\in j} p_\alpha\omega_{\alpha\beta} \right]
\end{equation}

Expanding gives:
\begin{equation}
    w_i = \sum_{\alpha\in j} \sum_\beta p_\alpha\omega_{\alpha\beta} -\sum_{\alpha\in j}\sum_{\beta\in k} p_\alpha\omega_{\alpha\beta}
          +\sum_{\alpha\in k} \sum_\beta p_\alpha\omega_{\alpha\beta} -\sum_{\alpha\in k}\sum_{\beta\in j} p_\alpha\omega_{\alpha\beta}
\end{equation}
which by definition is
\begin{equation}
    w_i = w_j -w_{jk} +w_k -w_{kj}
\label{eq:w_i}
\end{equation}

This allows economical calculations.

We can do similar for \(w_{il}\), if we merged modules \(j\) and \(k\) into \(i\), and \(l\) is some other module:
\begin{align}
    w_{il} &= \sum_{\alpha\in i} \sum_{\beta\in l} p_\alpha\omega_{\alpha\beta} \\
           &= \sum_{\substack{~~~\alpha\in j\\\mathrm{or}~\alpha\in k}} \sum_{\beta\in l} p_\alpha\omega_{\alpha\beta} \\
           &= \sum_{\alpha\in j} \sum_{\beta\in l} p_\alpha\omega_{\alpha\beta} +\sum_{\alpha\in k} \sum_{\beta\in l} p_\alpha\omega_{\alpha\beta} \\
           &= w_{jl} +w_{kl}
\label{eq:w_il}
\end{align}
and similarly for \(w_{li}\):
\begin{equation}
    w_{li} = w_{lj} +w_{lk}
\label{eq:w_li}
\end{equation}

We can simplify further. The directionality of the connections are not needed, since \(w_{ij}\) and \(w_{ji}\) always appear together in Eq.~(\ref{eq:w_i}). Then, we can define
\begin{equation}
    w_{i\leftrightharpoons l} = w_{il} +w_{li}
\end{equation}
and we can verify that Eq.~(\ref{eq:w_il}) and Eq.~(\ref{eq:w_li}) combine to give
\begin{align}
    w_{i\leftrightharpoons l} &= w_{il} +w_{li} \\
    &= w_{jl} +w_{kl} +w_{lj} +w_{lk} \\
    &= w_{jl} +w_{lj} +w_{kl} +w_{lk} \\
    &= w_{j\leftrightharpoons l} +w_{k\leftrightharpoons l}
\end{align}
and this quantity is applied via
\begin{align}
    q_i &= \tau\frac{n-n_i}{n-1}p_i +(1-\tau)w_i \\
    w_i &= w_j +w_k -w_{j\leftrightharpoons l}
\label{eq:q-simplest}
\end{align}

The calculations above has a key, central message: that \textbf{for the purpose of community detection, we can forget about the actual nodal properties; after each merge, we only need to keep track of a module/community}.

\subsection{Calculating code length reduction}

Given an initial code length according to Eq.~(\ref{eq:L}), further iterations in code length calculation can calculated via dynamic programming. Whenever we merge two modules \(j\) and \(k\) into \(i\), with new module frequency \(p_i\) and \(q_i\), the change in code length is:
\begin{align}
    \Delta L_i &= \mathrm{plogp}\left[ q_i-q_j-q_k+\sum_i q_i \right] -\mathrm{plogp} \left[ \sum_i q_i \right] \nonumber\\
    &-2 \mathrm{plogp}(q_i) +2\mathrm{plogp}(q_j) +2\mathrm{plogp}(q_k) \nonumber\\
    &+\mathrm{plogp}(p_i+q_i) -\mathrm{plogp}(p_j+q_j) -\mathrm{plogp}(p_k+q_k)
\label{eq:DeltaL}
\end{align}
so that if we keep track of \(\sum_iq_i\), we can calculate \(\Delta L\) quickly by plugging in \(p_i\), \(p_j\), \(p_k\), \(q_i\), \(q_j\), \(q_k\).

\section{InfoMap Algorithm}

The algorithm consists of two stages, the initial condition and the loop:

\subsection{Initial Condition}

Each node is its own module, so that we have:
\begin{align*}
    n_i &= 1\\
    p_i &= p_\alpha \\
    w_i &= p_\alpha\sum_{\beta\neq\alpha}\omega_{\alpha\beta} \\
    q_i &= \tau\frac{n-n_i}{n-1}p_i +(1-\tau)w_i \\
    w_{i\leftrightharpoons j} &= \omega_{ij} +\omega_{ji}, ~~~\forall \omega_{ij}~\mathrm{and}~\omega_{ji}
\end{align*}
and \(\Delta L\) is calculated for all possible merging pairs according to Eq.~(\ref{eq:DeltaL}).

\subsection{Loop}

Find the merge pairs that would minimize the code length; if the code length cannot be reduced then terminate the loop. Otherwise, merge the pair to form a module with the following quantities, so that if we merge modules \(j\) and \(k\) into \(i\), then: (these equations are presented in previous sections, but now repeated for ease of reference)
\begin{align*}
    n_i &= n_j +n_k \\
    p_i &= p_j +p_k \\
    w_i &= w_j +w_k -w_{j\leftrightharpoons k} \\
    q_i &= \tau\frac{n-n_i}{n-1}p_i +(1-\tau)w_i \\
    w_{i\leftrightharpoons l} &= w_{j\leftrightharpoons l} +w_{k\leftrightharpoons l}, ~~~\forall l\neq i
\end{align*}
and
\begin{align}
    \Delta L_{i\leftrightharpoons l} &= \mathrm{plogp}\left[ q_{i\leftrightharpoons l}-q_i-q_l+\sum_i q_i \right] -\mathrm{plogp} \left[ \sum_k q_k \right] \nonumber\\
    &-2 \mathrm{plogp}(q_{i\leftrightharpoons l}) +2\mathrm{plogp}(q_i) +2\mathrm{plogp}(q_l) \nonumber\\
    &+\mathrm{plogp}(p_i+q_{i\leftrightharpoons l}) -\mathrm{plogp}(p_i+q_i) -\mathrm{plogp}(p_l+q_l)
\end{align}
    is recalculated for all merging pairs that involve module \(i\), i.e., for each \(w_{i\leftrightharpoons l}\). The sum \(\sum_iq_i\) is iterated in each loop by adding \(q_i-q_j-q_k\).

\subsection{Algorithm}

Given the above math, the pseudocode is:

Initiation:
\begin{itemize}
    \item Calculate the RDD of edges in the format ((vertex1,vertex2),weight). Vertex1 and vertex2 are arranged such that vertex1 is always smaller than vertex2. If the directed edge vertex1 to vertex2 exists and the directed edge vertex2 to vertex1 exists, then the weight of the RDD entry is the sum of the two directed edge weights. Otherwise, the weight of the RDD entry is the weight of the existing edge.
    \item Construct a table (i.e. row and column), where each row is an undirected edge between modules in the graph. Each row is of format ( (vertex1,vertex2), ( n1, n2, p1, p2, w1, w2, w12, q1, q2, \(\Delta\)L ) ). The quantities n, p, w, q, are properties of the two modules, n is the nodal size of the module, p is the ergodic frequency of the module, w is the exit probability of a module without teleportation, q is the exit probability of a module with teleportation, \(\Delta\)L is the change in code length if the two modules are merged. Vertex1 and vertex2 are arranged such that vertex1 is always smaller than vertex2.
\end{itemize}

Loop:
\begin{enumerate}
    \item Pick the row that has the smallest \(\Delta\)L. If it is non-negative, terminate the algorithm.
    \item Calculate the newly merged module size, ergodic frequency, exit probabilities.
    \item Calculate the new RDD of edges by deleting the edges between the merging modules, and then aggregate all edges associated with module 2 to those in module 1.
    \item Update the table by aggregating all rows associated with module 2 to those in module 1. Join the table with the RDD of edges. Since the RDD of edges contain \(w_{1\leftrightharpoons k}\), we can now calculate \(\Delta\)L and put it in the table for all rows associated with module 1.
    \item Repeat from Step 1.
\end{enumerate}

There are \(O(e)\) merges, e being the number of edges in the graph.

\section{Merging multiple modules at once}

In the previous sections, we have developed the discrete mathematics and algorithm that performs community detection with \(O(e)\) loops, based on the key mathematical finding that we only need to remember modular properties, not nodal ones.

The algorithm above does not take advantage of the parallel processing capabilities of Spark. One obvious improvement possibility is to perform multiple merges per loop. However, algorithms so far focus on merging two modules on each iteration, which is not compatible with the idea of performing multiple merges, unless we can make sure no module is involved with more than one merge at once.

Here, rather than focusing on making sure that no module is involved with more than one merge at once, we can explore the idea of merging multiple modules at once. Thus, we can perform parallel merges in the same loop iteration, where possibly \emph{all} modules are involved in some merge.

\subsection{Mathematics}

Here we develop the mathematics to keep track of merging multiple modules at once.

We consider multiple modules \(\mathcal{M}_i\) merging into a module \(\mathcal{M}\). Another way to express this equivalently is to say that a module \(\mathcal{M}\) is partitioned into \(i\) non-overlapping subsets:
\begin{equation}
    \mathcal{M} = \sum_i \mathcal{M}_i
\end{equation}

Then we can expand the nodal sum over module \(\mathcal{M}\) into the sum over all nodes in all submodules \(\mathcal{M}_i\), the exit probability of the merged module \(\mathcal{M}\) becomes:
\begin{align}
    w_\mathcal{M} &= \sum_{\mathcal{M}_i} \sum_{\alpha\in\mathcal{M}_i} \sum_{\beta\notin\mathcal{M}} p_\alpha w_{\alpha\beta} \\
    &= \sum_{\mathcal{M}_i} \sum_{\alpha\in\mathcal{M}_i} \left[ \sum_\beta p_\alpha w_{\alpha\beta} -\sum_{\beta\in\mathcal{M}} p_\alpha w_{\alpha\beta} \right] \\
    &= \sum_{\mathcal{M}_i} \sum_{\alpha\in\mathcal{M}_i} \sum_\beta p_\alpha w_{\alpha\beta} -\sum_{\mathcal{M}_i} \sum_{\alpha\in\mathcal{M}_i} \sum_{\beta\in\mathcal{M}} p_\alpha w_{\alpha\beta} \\
    &= \sum_{\mathcal{M}_i} \sum_{\alpha\in\mathcal{M}_i} \sum_\beta p_\alpha w_{\alpha\beta} -\sum_{\mathcal{M}_i} \sum_{\alpha\in\mathcal{M}_i} \sum_{\mathcal{M}_j}\sum_{\beta\in\mathcal{M}_j} p_\alpha w_{\alpha\beta}
\end{align}
where we expand the second term with respect to the \(\mathcal{M}_j\)'s to give
\begin{align}
    w_\mathcal{M} &= \sum_{\mathcal{M}_i} \sum_{\alpha\in\mathcal{M}_i} \sum_\beta p_\alpha w_{\alpha\beta} \\
    &-\sum_{\mathcal{M}_i} \sum_{\alpha\in\mathcal{M}_i} \sum_{\mathcal{M}_j\neq\mathcal{M}_i}\sum_{\beta\in\mathcal{M}_j} p_\alpha w_{\alpha\beta} -\sum_{\mathcal{M}_i} \sum_{\alpha\in\mathcal{M}_i} \sum_{\beta\in\mathcal{M}_i} p_\alpha w_{\alpha\beta}
\end{align}
Combining the first and third terms,
\begin{equation}
    w_\mathcal{M} = \sum_{\mathcal{M}_i} \sum_{\alpha\in\mathcal{M}_i} \sum_{\beta\notin\mathcal{M}_i} p_\alpha w_{\alpha\beta} -\sum_{\mathcal{M}_i} \sum_{\alpha\in\mathcal{M}_i} \sum_{\mathcal{M}_j\neq\mathcal{M}_i}\sum_{\beta\in\mathcal{M}_j} p_\alpha w_{\alpha\beta}
\end{equation}
which we can recognize as
\begin{align}
    w_\mathcal{M} &= \sum_{\mathcal{M}_i} w_{\mathcal{M}_i} -\sum_{\mathcal{M}_i} \sum_{\mathcal{M}_j\neq\mathcal{M}_i} w_{\mathcal{M}_i\mathcal{M}_j} \label{eq:multimerge-w} \\
    w_{\mathcal{M}_i} &= \sum_{\alpha\in\mathcal{M}_i} \sum_{\beta\notin\mathcal{M}_i} p_\alpha w_{\alpha\beta} \label{eq:multimerge-wi} \\
    w_{\mathcal{M}_i\mathcal{M}_j} &= \sum_{\alpha\in\mathcal{M}_i}\sum_{\beta\in\mathcal{M}_j} p_\alpha w_{\alpha\beta} \label{eq:multimerge-wij}
\end{align}
where we can immediately see that Eq.~(\ref{eq:multimerge-w}) is a linear generalization of Eq.~(\ref{eq:q-simplest}), while Eq.~(\ref{eq:multimerge-wi}) and Eq.~(\ref{eq:multimerge-wij}) are identical to previous definitions, and may be calculated iteratively as the previous algorithm. We can calculate \(w_{\mathcal{M}_i\mathcal{M}_j}\) by expanding on the partitioning:
\begin{align}
    w_{\mathcal{M}_i\mathcal{M}_j} &= \sum_{\mathcal{M}_k\in\mathcal{M}_i} \sum_{\alpha\in\mathcal{M}_k} \sum_{\mathcal{M}_{k'}\in\mathcal{M}_j} \sum_{\beta\in\mathcal{M}_j} p_\alpha w_{\alpha\beta}\\
    &= \sum_{\mathcal{M}_k\in\mathcal{M}_i} \sum_{\mathcal{M}_{k'}\in\mathcal{M}_j} w_{\mathcal{M}_k\mathcal{M}_{k'}}
\end{align}
so that when we merge a number of modules together, we can calculate its connections with other modules by aggregating the existing modular connections. This is directly analogous to Eq.~(\ref{eq:w_il}).

Thus, the mathematical properties of merging multiple modules into one are identical to that of merging two modules. This is key to developing multi-merge algorithm.

\subsection{Algorithm}

Initiation is similar to the infomap algorithm, so that we have an RDD of modular properties in the format (index,n,p,w,q), and edge properties ((vertex1,vertex2),summed connection weight), ((vertex1,vertex2),deltaL).

Loop:
\begin{enumerate}
    \item For each module, seek to merge with another module that would offer the greatest reduction in code length. If no such merge exists, the module does not seek to merge. Then, we have \(O(e)\) edges.
    \item For each of these edges, label it to the module index to be merged to, so that each connected component of the graph has the same label. This has \(O(k)\) complexity, \(k\) being the size of the connected component. The precise algorithm is described below.
    \item Recalculate modular and edge property values via aggregations:
        \begin{align*}
            n_i &= \sum_{k\rightarrow i} n_k\\
            p_i &= \sum_{k\rightarrow i} p_k\\
            w_i &= \sum_{k\rightarrow i} w_k -\sum_{k\leftrightharpoons k'\rightarrow i} w_{k\leftrightharpoons k'}\\
            q_i &= \tau\frac{n-n_i}{n-1}p_i +(1-\tau)w_i \\
            w_{i\leftrightharpoons j} &=
                \sum_{k\rightarrow i} \sum_{k'\rightarrow j} w_{k\leftrightharpoons k'} \\
            L &=
                \mathrm{plogp}\left( \sum_iq_i \right)
                -2\sum_i\mathrm{plogp}\left(q_i\right) \\
                & -\sum_\alpha \mathrm{plogp}(p_\alpha)
                +\sum_i\mathrm{plogp}\left( p_i+q_i \right) \\
            \Delta L_{i\leftrightharpoons j} &= \mathrm{plogp}\left[ q_{i\leftrightharpoons j}-q_i-q_j+\sum_k q_k \right] -\mathrm{plogp} \left[ \sum_k q_k \right] \\
    &-2 \mathrm{plogp}(q_{i\leftrightharpoons j}) +2\mathrm{plogp}(q_i) +2\mathrm{plogp}(q_j) \\
    &+\mathrm{plogp}(p_{i\leftrightharpoons j}+q_{i\leftrightharpoons j}) -\mathrm{plogp}(p_i+q_i) -\mathrm{plogp}(p_j+q_j)
    \end{align*}
    %\item Delete the rows in the table that contain the extra modules.
    %\item For the remaining modules and edges, calculate \(w\), \(q\) and \(\Delta L\).
\end{enumerate}

\subsubsection*{Labeling connected components}

This algorithm labels a graph so that all connected components are labeled by the same module index, the module index being the one that occurs the most frequently within the linked edges.

Initiation:
\begin{enumerate}
    \item Given the edges, count the occurrences of the vertices.
    \item Label each edge to one of the two vertices with the higher occurrence.
\end{enumerate}

Loop:
\begin{enumerate}
    \setcounter{enumi}{-1}
    \item Each edge is associated with a label, so that each vertex is associated with at least one label.
    \item Count the label occurrences associated with each vertex, so that for each vertex, we have a list of labels and its associated count.
    \item Re-associate each vertex with its max-count label.
    \item If no relabeling is needed, terminate.
    %\item Count the edge label occurrences.
    %\item For each edge, if both vertices have nonzero label counts, produce a map from vertex of low count to vertex of high count.
    %\item If we have an empty map, terminate loop. Otherwise, apply the map to the edge labels.
\end{enumerate}

\subsection{Performance Improvement}

Infomap performs greedy merges of two modules until no more merges can be performed. If there are \(n\) nodes in the graph and \(m\) modules in the end, then there are \(n-m\) merges, since we assume the graph is sparse and the edges are proportional to the number of nodes. Since each loop merges two modules, there are \(n-m-1\) loops. Thus, infomap has linear time complexity to the number of nodes/edges.

In the multiple merging scheme, within each loop each module merges with one other module. Let's assume in each loop, \(k\) modules merge into one on average. Then, let there be \(l\) loops, and as before, \(n\) nodes are merged into \(m\) modules. Since each loop reduces the amount of modules by \(k\) times, we have
\begin{align}
    n k^{-l} &= m \\
    k^l &= \frac{n}{m} \\
    l &= \mathrm{log}_k n -\mathrm{log}_k m
\end{align}
Within each merge, there are \(O(k)\) operations to aggregate the indices appropriately for the merges.

Thus, the overall time complexity is \(O(k~\mathrm{log}~n)\). For example, if \(k=2\), i.e., every pair of modules are merged every step, then we have \(O(2~\mathrm{log}~n)\) complexity, i.e., logarithmic complexity in the number of nodes/edges.

In the worst cases, we have linear time complexity, if \(l=1, k=n/m\), so that the overall complexity is \(O(k) = O(n/m)\). Another possibility is if InfoFlow degenerates into InfoMap, so that we merge a pair of modules every step, and of course recover linear complexity.

The central reason behind the logarithmic time complexity is the constant average merging of \(k\) modules into one. The constancy of this factor likely depends on the network structure. Given a sparse network, the number of edges is similar to the number of nodes. If we make the assumption that \(k\) is proportional to the number of edges, then after a loop, the number of modules is reduced by a factor of \(k\), and so is the number of edges. Thus, the network sparsity remains unchanged, and \(k\) is unchanged also. Of course, ultimately, actual performance benchmarks are required.

The logarithmic complexity and the \(k\) constant suggests that perhaps enforced binary merging, i.e., either pair-wise merge or no merge at all for some modules, might achieve best runtime complexity. A possible catch-22 might be that, to enforce pair-wise merges, \(O(k)\) explorations would be needed, so that the runtime complexity remains the same, and actual performance is even penalized. Further mathematical ideas, simulations and benchmarks would be required for further explorations.

\end{document}
